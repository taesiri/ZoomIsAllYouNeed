<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="Zoom is what you need: An empirical study of the power of zoom and spatial biases in image classification">
  <meta name="keywords" content="CLIP, ResNet50, ViT, Representation Learning, Transfer Learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Zoom is what you need </title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-573GPBEL8Q"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-573GPBEL8Q');
  </script>

  <link rel="stylesheet" href="https://gradio.s3-us-west-2.amazonaws.com/2.6.2/static/bundle.css">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels"></script>

  <style>
    ol li {
      list-style: none;
      position: relative;
    }

    ol.alpha {
      counter-reset: alpha
    }

    ol.alpha>li:before {
      counter-increment: alpha;
      content: "(" counter(alpha, lower-alpha)") "
    }

    .image-grid {
      display: grid;
      grid-template-columns: repeat(4, 1fr);
      grid-template-rows: repeat(3, 1fr);
      /* Added to set equal height rows */
      grid-gap: 10px;
    }

    .image-grid-item {
      display: flex;
      flex-direction: column;
      align-items: center;
    }

    .image-grid-text {
      display: flex;
      justify-content: space-between;
      width: 100%;
      margin-bottom: 5px;

    }

    .image-grid-text-left {
      text-align: left;
      width: 50%;
    }

    .image-grid-text-right {
      text-align: right;
      width: 50%;
    }

    .correct-prediction {
      color: green;
    }

    .wrong-prediction {
      color: red;
    }

    .grid-container {
      display: flex;
      height: calc(100% - 20px);
      /* Add this rule */
    }

    .category-column {
      display: flex;
      flex-direction: column;
      justify-content: space-around;
      /* Change from space-between to space-around */
      margin-right: 15px;
      /* Add this rule */
    }

    .category-text {
      writing-mode: vertical-rl;
      transform: rotate(180deg);
      text-align: center;
      margin-top: 40px;
      /* Add this rule to adjust the position */
    }
  </style>
</head>

<body>
  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    </div>
  </nav>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ImageNet-Hard: The Hardest Images Remaining from a Study of the
              Power of Zoom <img src="./static/svg/magnifying-glass-plus-solid.svg" width="50" height="23"> and Spatial
              Biases in Image Classification</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://taesiri.com">Mohammad Reza Taesiri</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://giangnguyen2412.github.io/">Giang Nguyen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://habchisarra.github.io/">Sarra Habchi
                </a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://asgaard.ece.ualberta.ca/">Cor-Paul Bezemer</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://anhnguyen.me/research/">Anh Nguyen</a><sup>2</sup>,
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Alberta,</span>
              <span class="author-block"><sup>2</sup>Auburn University,</span>
              <span class="author-block"><sup>3</sup>Ubisoft</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2304.05538.pdf"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2304.05538" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/taesiri/ZoomIsAllYouNeed"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/taesiri/imagenet-hard"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset</span>
                  </a>
                </span>

              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">

            <p>
              Image classifiers are information-discarding machines, by design. Yet, how these models discard
              information remains mysterious.
              We hypothesize that one way for image classifiers to reach high accuracy is to first zoom to the most
              discriminative region in the image and then extract features from there to predict image labels,
              discarding the rest of the image.
              Studying six popular networks ranging from <a
                href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">AlexNet</a>
              to <a href="https://arxiv.org/abs/2103.00020">CLIP</a>, we find that proper framing of the input image can
              lead to the correct classification of 98.91% of <a href="https://www.image-net.org/">ImageNet</a> images.
              Furthermore, we uncover positional biases in various datasets, especially a strong center bias in two
              popular datasets: <a href="https://github.com/hendrycks/natural-adv-examples">ImageNet-A </a>and <a
                href="https://objectnet.dev/">ObjectNet</a>.
              Finally, leveraging our insights into the potential of zooming, we propose a test-time augmentation (TTA)
              technique that improves classification accuracy by forcing models to explicitly perform zoom-in operations
              before making predictions.
              Our method is more interpretable, accurate, and faster than <a
                href="https://arxiv.org/abs/2110.09506">MEMO</a>, a state-of-the-art (SOTA) TTA method.
              We introduce <strong>ImageNet-Hard</strong>, a new benchmark that challenges SOTA classifiers
              including large
              vision-language models even when optimal zooming is allowed.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <!-- <h2 class="title is-3">Zoom</h2> -->
        </div>
      </div>
      <div class="grid-container">
        <div class="category-column">
          <div class="category-text">ImageNet-A</div>
          <div class="category-text">ImageNet-Sketch</div>
          <div class="category-text">ImageNet-ReaL</div>
        </div>
        <div class="image-grid">
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left wrong-prediction">flagpole</span>
              <span class="image-grid-text-right">30.47%</span>
            </div>
            <img src="./static/images/image1.png" alt="Image 1">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left wrong-prediction">robin</span>
              <span class="image-grid-text-right">72.93%</span>
            </div>
            <img src="./static/images/image2.png" alt="Image 2">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left correct-prediction">junco</span>
              <span class="image-grid-text-right">49.06%</span>
            </div>
            <img src="./static/images/image3.png" alt="Image 3">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left correct-prediction">junco</span>
              <span class="image-grid-text-right">94.98%</span>
            </div>
            <img src="./static/images/image4.png" alt="Image 4">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left wrong-prediction">electric fan</span>
              <span class="image-grid-text-right">67.10%</span>
            </div>
            <img src="./static/images/image5.png" alt="Image 5">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left wrong-prediction">electric fan</span>
              <span class="image-grid-text-right">67.10%</span>
            </div>
            <img src="./static/images/image6.png" alt="Image 6">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left correct-prediction">hornbill</span>
              <span class="image-grid-text-right">10.55%</span>
            </div>
            <img src="./static/images/image7.png" alt="Image 7">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left correct-prediction">hornbill</span>
              <span class="image-grid-text-right">89.14%</span>
            </div>
            <img src="./static/images/image8.png" alt="Image 8">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left wrong-prediction">impala</span>
              <span class="image-grid-text-right">65.63%</span>
            </div>
            <img src="./static/images/image9.png" alt="Image 9">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left wrong-prediction">gazelle</span>
              <span class="image-grid-text-right">40.51%</span>
            </div>
            <img src="./static/images/image10.png" alt="Image 10">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left correct-prediction">magpie</span>
              <span class="image-grid-text-right">60.23%</span>
            </div>
            <img src="./static/images/image11.png" alt="Image 11">
          </div>
          <div class="image-grid-item">
            <div class="image-grid-text">
              <span class="image-grid-text-left correct-prediction">magpie</span>
              <span class="image-grid-text-right">83.78%</span>
            </div>
            <img src="./static/images/image12.png" alt="Image 12">
          </div>
        </div>
      </div>
      <br>
      <p>
        Each subfigure shows an input image, predicted label, and confidence score from an ImageNet-trained classifier
        (top and middle: <a href="https://arxiv.org/abs/1512.03385">ResNet-50</a>; bottom: <a
          href="https://arxiv.org/abs/2010.11929">ViT-B/32</a>).
        With the standard center-crop image transform, all 3 samples were <span
          class="wrong-prediction">misclassified</span> (left-most column).
        Adjusting the framing by zooming in or out leads to <span class="correct-prediction">correct</span> predictions.
      </p>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <img src="./static/gifs/animation_zoom_in.gif" alt="animation" width="90%" height="100%">
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <div class="content">
            <p>
              Enhancing the clarity of an object by zooming in often results in correct classification.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column">
          <img src="./static/gifs/animation_zoom_out.gif" alt="animation" width="90%" height="100%">
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths has-text-centered">
          <div class="content">
            <p>
              Proper framing of an object through zooming out can enhance the accuracy of its classification.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Method Overview</h2>
          <div class="content has-text-justified">
            <p>
              Our method involves a zooming process that combines resizing and cropping operations. First, we resize the
              image so that its smaller dimension matches a target scale (S), and then divide it into 3x3 patches. A
              224x224px patch is extracted from the center of each patch, with zero padding added if necessary. We
              demonstrate that this simple approach is sufficient to solve up to 99.44% of the images in the ImageNet
              dataset.
            </p>
          </div>
          <img src="./static/svg/overview.svg" alt="Zooming" style="width: 100%; height: auto;">
          <p>
            Overview of the process
          </p>
          <div class="column">
            <img src="./static/gifs/animation-1.gif" alt="animation" width="90%" height="100%">
            <p>
              Demonstration of how different image patches change when we alter the zoom scale.
            </p>
          </div>
          <div class="column">
            <img src="./static/gifs/animation-2.gif" alt="animation" width="90%" height="100%">
            <p>
              Demonstration of the use of nine distinct crops to capture different regions of an image from the
              ImageNet-A dataset. The ground truth, a <span
                style="font-style: italic; background-color:rgb(242,242,242)">mallard</span>, is situated at the bottom
              of the image. While center cropping fails to classify the bird correctly, crops at the bottom allow for
              accurate classification. </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Results</h2>
          <div class="content has-text-justified">
            <p>
              To understand the potential of using zooming to improve image classification accuracy, first, we establish
              an upper bound (i.e., when an "optimal" zoom is given). That is, we apply 36 scales * 9 anchors = 324 zoom
              transformations to each image to generate 324 zoomed versions of the same input. We then feed
              all 324 versions to each network and label an image "correctly classified given the optimal zoom" if at
              least 1 of the 324 is correctly labeled.

              This experiment also importantly informs the community of the type of image that cannot be correctly
              labeled even with an optimal zooming strategy.
            </p>


            <p>
              On in-distribution data (ImageNet & ImageNet-ReaL) there exists a substantial improvement when models are
              provided with
              an optimal zoom, either selected from 36 (b) or 324 pre-defined zoom crops (c).
              In contrast, <span style="background-color:rgb(242,242,242)">OOD benchmarks</span> still pose a
              significant challenge to
              <span style="color:rgb(160, 82, 45)">IN-trained models</span> even with optimal zooming (i.e.,
              all upper-bound accuracy
              scores < 80%). </p>

                <img src="./static/svg/Table1.svg" alt="Zooming" style="width: 100%; height: auto;">
          </div>
          <br>
          <div class="content has-text-justified">

            <p>State-of-the-art test-time augmentation methods, such as MEMO, finetune a pre-trained classifier at
              <em>test</em> time
              to achieve a more accurate prediction. Specifically, MEMO attempts to find a network that produces a
              low-entropy predicted label over a set of <em>K</em>=16 augmented versions of the test image <em>I</em>
              and then runs this finetuned model on <em>I</em> again to produce the final prediction. While improving
              accuracy, MEMO requires a pre-defined set of diverse augmentation transforms (e.g., sheer, rotate, and
              solarize in <a href="https://arxiv.org/abs/1912.02781">AugMix</a>). Yet, the hyperparameters
              for each type of transform are hard-coded, and the reasons why some of such transform help classification
              are unknown.
            </p>
            <p>Our goal is to improve MEMO's accuracy and interpretability by replacing all AugMix transforms with only
              zoom-in transforms. Intuitively, we let a classifier first look at all zoomed-in frames of the input image
              (at different zoom scales and locations) and then make up its mind to achieve the most confident
              prediction.</p>

          </div>
          <br>
          <div class="content has-text-justified">
            <p>
              MEMO + <span style="font-style: italic;">RRC</span> (i.e., random zoom-in transforms) <span
                style="font-weight: bold">outperforms</span>
              baselines and the
              default MEMO.
            </p>

            <img src="./static/svg/Table2.svg" alt="Zooming" style="width: 100%; height: auto;">
          </div>

          <div class="columns is-multiline">
            <div class="column is-half">
              <figure class="image">
                <img src="./static/images/gradcammemo-1.png" alt="Memo Result 1" style="max-width: 100%; height: auto;">
              </figure>
            </div>
            <div class="column is-half">
              <figure class="image">
                <img src="./static/images/gradcammemo-2.png" alt="Memo Result 2" style="max-width: 100%; height: auto;">
              </figure>
            </div>
            <div class="column is-half">
              <figure class="image">
                <img src="./static/images/gradcammemo-3.png" alt="Memo Result 3" style="max-width: 100%; height: auto;">
              </figure>
            </div>
            <div class="column is-half">
              <figure class="image">
                <img src="./static/images/gradcammemo-4.png" alt="Memo Result 4" style="max-width: 100%; height: auto;">
              </figure>
            </div>
            <div class="column is-half">
              <figure class="image">
                <img src="./static/images/gradcammemo-5.png" alt="Memo Result 5" style="max-width: 100%; height: auto;">
              </figure>
            </div>
            <div class="column is-half">
              <figure class="image">
                <img src="./static/images/gradcammemo-6.png" alt="Memo Result 6" style="max-width: 100%; height: auto;">
              </figure>
            </div>
          </div>


        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">ImageNet-Hard</h2>
          <div class="content has-text-justified">
            We present ImageNet-Hard, a novel benchmark in which merely zooming in does not enhance the performance of
            state-of-the-art models in accurately labeling images.
            The ImageNet-Hard is a new benchmark that comprises 11,350 images, collected from various existing
            ImageNet-scale benchmarks (ImageNet, ImageNet-V2, ImageNet-Sketch, ImageNet-C, ImageNet-R, ImageNet-ReaL,
            ImageNet-A, and
            ObjectNet). This dataset is challenging to state-of-the-art vision models, as merely zooming in often fails
            to enhance their ability to classify images correctly. Consequently, even the most advanced models, such as
            CLIP-ViT-L/14@336px, struggle to perform well on this dataset, achieving only 2.30% accuracy.
          </div>
          <div>
            <h4 class="title is-6">Dataset Distribution</h4>
            <div>
              <img src="./static/svg/imagenet_hard_distribution.svg" alt="Zooming" style="width: 100%; height: auto;">
            </div>
          </div>
          <br>
          <div>
            <h4 class="title is-6">Performance Report</h4>
            <canvas id="barChart"></canvas>
          </div>
          <br>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">Citation information</h2>
      <pre><code>@article{taesiri2023zoom,
        title={ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification},
        author={Taesiri, Mohammad Reza and Nguyen, Giang and Habchi, Sarra and Bezemer, Cor-Paul and Nguyen, Anh},
        journal={arXiv preprint arXiv:2304.05538},
        year={2023}
      }
        </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is forked from the
              <a href="https://nerfies.github.io/">Nerfies website</a> and
              <a href="https://github.com/nerfies/nerfies.github.io">source code</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>


  <script>
    document.addEventListener('DOMContentLoaded', function () {
      const results = {
        'AlexNet': 7.34,
        'VGG-16': 12.00,
        'ResNet-18': 10.86,
        'ResNet-50': 14.74,
        'ViT-B/32': 18.52,
        'CLIP-ViT-L/14': 1.86,
        'CLIP-ViT-L/14@336px': 2.02,
        'EfficientNet-B0': 16.58,
        'EfficientNet-L2-Ns': 39.00,
        'OpenCLIP-ViT-bigG-14': 15.93,
        'OpenCLIP-ViT-L-14': 15.60
      };

      const labels = Object.keys(results);
      const data = Object.values(results);

      const ctx = document.getElementById('barChart').getContext('2d');
      const chart = new Chart(ctx, {
        type: 'bar',
        data: {
          labels: labels,
          datasets: [{
            label: 'Top-1 Accuracy',
            data: data,
            backgroundColor: [
              'rgba(75, 192, 192, 0.2)',
              'rgba(255, 206, 86, 0.2)',
              'rgba(255, 99, 132, 0.2)',
              'rgba(54, 162, 235, 0.2)',
              'rgba(153, 102, 255, 0.2)',
              'rgba(255, 159, 64, 0.2)',
              'rgba(201, 203, 207, 0.2)',
              'rgba(75, 192, 192, 0.2)',
              'rgba(0, 123, 255, 0.2)',
              'rgba(255, 105, 180, 0.2)',
              'rgba(34, 139, 34, 0.2)'
            ],
            borderColor: [
              'rgba(75, 192, 192, 1)',
              'rgba(255, 206, 86, 1)',
              'rgba(255, 99, 132, 1)',
              'rgba(54, 162, 235, 1)',
              'rgba(153, 102, 255, 1)',
              'rgba(255, 159, 64, 1)',
              'rgba(201, 203, 207, 1)',
              'rgba(75, 192, 192, 1)',
              'rgba(0, 123, 255, 1)',
              'rgba(255, 105, 180, 1)',
              'rgba(34, 139, 34, 1)'
            ],
            borderWidth: 1
          }]
        },
        options: {
          plugins: {
            legend: {
              labels: {
                usePointStyle: true
              }
            }
          },
          scales: {
            y: {
              beginAtZero: true
            }
          }
        }
      });
    });
  </script>




</body>

</html>